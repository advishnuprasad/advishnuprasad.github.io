<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Elasticsearch | A D Vishnu Prasad]]></title>
  <link href="https://advishnuprasad.com/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="https://advishnuprasad.com/"/>
  <updated>2019-09-27T00:28:25+05:30</updated>
  <id>https://advishnuprasad.com/</id>
  <author>
    <name><![CDATA[A D Vishnu Prasad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kubernetes- Elasticsearch Scheduled Backup and Retention Using Curator]]></title>
    <link href="https://advishnuprasad.com/blog/2018/10/07/kubernetes-elasticsearch-scheduled-backup-and-retention-using-curator/"/>
    <updated>2018-10-07T21:54:58+05:30</updated>
    <id>https://advishnuprasad.com/blog/2018/10/07/kubernetes-elasticsearch-scheduled-backup-and-retention-using-curator</id>
    <content type="html"><![CDATA[<p>This post will guide you to create a scheduled elasticsearch backup using curator in k8s cluster.</p>

<h4>Assumptions:</h4>

<pre><code>You already have a 6.x.x elasticsearch k8s cluster
You already have a backup system
You are some what familiar with Kubernetes
You are familiar with curator configs
</code></pre>

<h4>Step 1: Create a configmap with curator yml</h4>

<pre><code>Action 1 will create a snapshot
Action 2 will keep last 20 days snapshots and delete that are older than 20 days
</code></pre>

<p>curator.yml
<code>
apiVersion: v1
kind: ConfigMap
metadata:
  name: curator-config
  labels:
    app: curator
data:
  backup.yml: |-
    ---
    # Remember, leave a key empty if there is no value.  None will be a string,
    # not a Python "NoneType"
    #
    # Also remember that all examples have 'disable_action' set to True.  If you
    # want to use this action as a template, be sure to set this to False after
    # copying it.
    actions:
      1:
        action: snapshot
        description: "Create snapshot"
        options:
          repository: "primary_backup"
          continue_if_exception: False
          wait_for_completion: True
        filters:
          - filtertype: pattern
            kind: regex
            value: ".*$"
      2:
        action: delete_snapshots
        description: &gt;-
          Delete snapshots from the selected repository older than 10 days
          (based on creation_date), for 'curator-' prefixed snapshots.
        options:
          repository: "primary_backup"
          disable_action: False
        filters:
          - filtertype: pattern
            kind: prefix
            value: curator-
            exclude:
          - filtertype: age
            source: creation_date
            direction: older
            unit: days
            unit_count: 20
  config.yml: |-
    ---
    # Remember, leave a key empty if there is no value.  None will be a string,
    # not a Python "NoneType"
    client:
      hosts:
        - elasticsearch
      port: 9200
      url_prefix:
      use_ssl: False
      certificate:
      client_cert:
      client_key:
      ssl_no_validate: False
      http_auth:
      timeout: 30
      master_only: False
    logging:
      loglevel: DEBUG
</code></p>

<h4>Step 2: Create a config map in your kubernetes cluster</h4>

<pre><code>kubectl apply -f curator.yml
</code></pre>

<h4>Step 3: Create a cronjob</h4>

<p>es_cronjob.yml</p>

<pre><code>apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: esbackup
  labels:
    app: esbackup
spec:
  schedule: "@daily"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 3
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 120
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - image: bobrik/curator:5.5.4
            name: curator
            args: ["--config", "/etc/config/config.yml", "/etc/config/backup.yml"]
            volumeMounts:
            - name: config
              mountPath: /etc/config
          volumes:
          - name: config
            configMap:
              name: curator-config
          restartPolicy: Never
</code></pre>

<h4>Step 4: Create cronjob in kubernetes</h4>

<p>This cronjob will run everyday 00:00 hrs
<code>
kubectl create -f es_cronjob.yml
</code></p>

<p>Reference:</p>

<ul>
<li><a href="https://medium.com/@hagaibarel/running-curator-as-a-kubernetes-cronjob-19eaab9afd3b">https://medium.com/@hagaibarel/running-curator-as-a-kubernetes-cronjob-19eaab9afd3b</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/delete_snapshots.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/delete_snapshots.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Autocomplete Example]]></title>
    <link href="https://advishnuprasad.com/blog/2018/09/23/elasticsearch-autocomplete-example/"/>
    <updated>2018-09-23T20:57:23+05:30</updated>
    <id>https://advishnuprasad.com/blog/2018/09/23/elasticsearch-autocomplete-example</id>
    <content type="html"><![CDATA[<p>This post will help you to create autocomplete feature on Elasticsearch.</p>

<p>For Example, if you have a select box and when you search for <code>data</code> you would want to get all results that starts with <code>data</code>.</p>

<p><img src="/images/blog_1.png" alt="ES1" /></p>

<!-- ![ES1](http://localhost:4000/images/blog_1.png) -->


<p>This can be done easily in Elasticsearch. The important thing here is the type of analyzer and tokenizer than you use for autocomplete.</p>

<h4>Step 1: Create Mapping with the following tokenizer and analyzer</h4>

<pre><code>PUT auto-test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "autocomplete": {
          "tokenizer": "autocomplete",
          "filter": [
            "lowercase"
          ]
        },
        "autocomplete_search": {
          "tokenizer": "lowercase"
        }
      },
      "tokenizer": {
        "autocomplete": {
          "type": "edge_ngram",
          "min_gram": 2,
          "max_gram": 10,
          "token_chars": [
            "letter"
          ]
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "technology": {
          "type": "text",
          "analyzer": "autocomplete",
          "search_analyzer": "autocomplete_search"
        }
      }
    }
  }
}
</code></pre>

<p><code>autocomplete_search</code> analyzer is used for searching case insensitive words.</p>

<p>Step 2: Test Analyzers</p>

<pre><code>POST auto-test/_analyze
{
  "field": "technology",
  "text": "database"
}
</code></pre>

<p>If you see the results, ES is creating the following tokens. You can change the min_gram based on your needs.</p>

<pre><code>{
  "tokens": [
    {
      "token": "da",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "dat",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 1
    },
    {
      "token": "data",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 2
    },
    {
      "token": "datab",
      "start_offset": 0,
      "end_offset": 5,
      "type": "word",
      "position": 3
    },
    {
      "token": "databa",
      "start_offset": 0,
      "end_offset": 6,
      "type": "word",
      "position": 4
    },
    {
      "token": "databas",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 5
    },
    {
      "token": "database",
      "start_offset": 0,
      "end_offset": 8,
      "type": "word",
      "position": 6
    }
  ]
}
</code></pre>

<p>Step 3: Create data and search</p>

<pre><code>POST auto-test/test
{
  "technology": "data"
}

POST auto-test/test
{
  "technology": "database"
}
</code></pre>

<pre><code>GET auto-test/_search
{
  "query": {
    "match": {
      "technology": "dat"
    }
  }
}
</code></pre>

<p>The results should be</p>

<pre><code>{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 0.2876821,
    "hits": [
      {
        "_index": "auto-test",
        "_type": "test",
        "_id": "3woWB2YBsF1yfhimkgKz",
        "_score": 0.2876821,
        "_source": {
          "technology": "data"
        }
      },
      {
        "_index": "auto-test",
        "_type": "test",
        "_id": "xGsWB2YBVxVaheWYfgCc",
        "_score": 0.2876821,
        "_source": {
          "technology": "database"
        }
      }
    ]
  }
}
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Backup and Restore From Azure Blob Storage]]></title>
    <link href="https://advishnuprasad.com/blog/2017/11/09/elasticsearch-backup-and-restore-from-azure-blob-storage/"/>
    <updated>2017-11-09T13:00:26-05:00</updated>
    <id>https://advishnuprasad.com/blog/2017/11/09/elasticsearch-backup-and-restore-from-azure-blob-storage</id>
    <content type="html"><![CDATA[<p>Assumptions:</p>

<p>You already have a working ELK cluster (5.x).</p>

<p>Azure Account</p>

<h3>Step 1: Storage account</h3>

<p>Create Storage Account:</p>

<p><img src="/images/es1.jpg" alt="ES1" /></p>

<!-- ![ES1](http://localhost:4000/images/es1.jpg)  -->


<h3>Step 2: Get Credentials</h3>

<p>Get Storage account name and key</p>

<p><img src="/images/es2.jpg" alt="ES2" /></p>

<!-- ![ES2](http://localhost:4000/images/es2.jpg)  -->


<h3>Step 3: Install azure plugin</h3>

<p>Ssh into all elastic search nodes.</p>

<p>Go to <code>/usr/share/elasticsearch/</code></p>

<p>Run <code>sudo bin/elasticsearch-plugin install repository-azure</code></p>

<h3>Step 4: Update config</h3>

<p>Go to <code>/etc/elasticsearch/elasticsearch.yml</code>. Add your Azure configuration
<img src="/images/es3.jpg" alt="ES3" /></p>

<!-- ![ES3](http://localhost:4000/images/es3.jpg)  -->


<p>Restart <code>sudo service elasticsearch restart</code></p>

<h3>Step 5: Create snapshots</h3>

<p>Open Kibana portal and Click on <code>Dev Tools</code></p>

<p>Configure Repository
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>PUT _snapshot/es_snapshot
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">type</span><span class="p">&amp;</span>rdquo<span class="p">;</span>: <span class="p">&amp;</span>ldquo<span class="p">;</span>azure<span class="p">&amp;</span>rdquo<span class="p">;</span>
</span><span class='line'><span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>Create Backup
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;PUT _snapshot/es_snapshot/mybackup_1
</span></code></pre></td></tr></table></div></figure></p>

<p>List snapshots
<code>bash
GET /_snapshot/es_snapshot/mybackup_1
</code></p>

<h3>Step 6:</h3>

<p>Go to Storage account. Click on &ldquo;Containers&rdquo; to see the snapshots.</p>

<p><img src="/images/es4.jpg" alt="ES4" /></p>

<!-- ![ES4](http://localhost:4000/images/es4.jpg)  -->


<h2>Restore from Azure storage account</h2>

<h3>Step 7:</h3>

<p>Follow step 1 to step 4 to configure your new cluster.</p>

<h3>Step 8:</h3>

<p>Close all the indices
<code>
POST /_all/_close
</code></p>

<h3>Step 9:</h3>

<p>Restore from snapshot</p>

<pre><code>POST /_snapshot/es_snapshot/mybackup_1/_restore
</code></pre>
]]></content>
  </entry>
  
</feed>
