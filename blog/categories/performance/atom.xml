<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | A D Vishnu Prasad]]></title>
  <link href="http://advishnuprasad.github.io/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://advishnuprasad.github.io/"/>
  <updated>2024-02-23T12:00:53+05:30</updated>
  <id>http://advishnuprasad.github.io/</id>
  <author>
    <name><![CDATA[A D Vishnu Prasad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Locust in Kubernetes for Performance Testing: A Powerful Combination]]></title>
    <link href="http://advishnuprasad.github.io/blog/2024/02/23/locust-in-kubernetes/"/>
    <updated>2024-02-23T11:48:08+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2024/02/23/locust-in-kubernetes</id>
    <content type="html"><![CDATA[<p>Performance testing ensures your applications gracefully handle the anticipated—and even the unanticipated—demands of real-world use. Locust, an open-source load testing framework, and Kubernetes, a container orchestration system, offer a potent mix for streamlining and scaling your performance testing efforts.</p>

<h3>Why Locust?</h3>

<p><strong>Python Power</strong>: Locust uses Python to define user behavior, making test creation intuitive and flexible. If you can code it, Locust can simulate it.</p>

<p><strong>Distributed Testing</strong>: Effortlessly simulate massive numbers of concurrent users by running Locust in a distributed mode.</p>

<p><strong>Elegant Web UI</strong>: Monitor your tests in real-time, gain deep insights into performance metrics, and identify bottlenecks thanks to Locust&rsquo;s user-friendly interface.</p>

<h3>Why Kubernetes?</h3>

<p><strong>Scalability</strong>: Seamlessly provision and manage the resources your Locust deployment needs. Spin up workers as required and scale down when testing is complete.</p>

<p><strong>Resilience</strong>: Kubernetes protects your test environment. If a worker node goes down, it automatically restarts pods, minimizing test disruption.</p>

<p><strong>Portability</strong>: Replicate your Locust test infrastructure across different environments (testing, staging, production) with ease.</p>

<p><img src="/images/locust.jpg" alt="Locust UI" /></p>

<h3>Step 1 : Create kubernetes manifests</h3>

<pre><code>---
apiVersion: v1
kind: ConfigMap
metadata:
  name: locust-script-cm
data:
  locustfile.py: |
    from locust import HttpUser, between, task
    import time


    class Quickstart(HttpUser):
        wait_time = between(1, 5)

        @task
        def google(self):
            self.client.request_name = "google"
            self.client.get("https://google.com/")

        @task
        def microsoft(self):
            self.client.request_name = "microsoft"
            self.client.get("https://microsoft.com/")

        @task
        def facebook(self):
            self.client.request_name = "facebook"
            self.client.get("https://facebook.com/")
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  labels:
    role: locust-master
    app: locust-master
  name: locust-master
spec:
  replicas: 1
  selector:
    matchLabels:
      role: locust-master
      app: locust-master
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        role: locust-master
        app: locust-master
    spec:
      containers:
      - image: locustio/locust
        imagePullPolicy: Always
        name: master
        args: ["--master"]
        volumeMounts:
          - mountPath: /home/locust
            name: locust-scripts
        ports:
        - containerPort: 5557
          name: bind
        - containerPort: 5558
          name: bind-1
        - containerPort: 8089
          name: web-ui
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
      - name: locust-scripts
        configMap:
          name: locust-script-cm
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  labels:
    role: locust-worker
    app: locust-worker
  name: locust-worker
spec:
  replicas: 1 # Scale it as per your need
  selector:
    matchLabels:
      role: locust-worker
      app: locust-worker
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        role: locust-worker
        app: locust-worker
    spec:
      containers:
      - image: locustio/locust
        imagePullPolicy: Always
        name: worker
        args: ["--worker", "--master-host=locust-master"]
        volumeMounts:
          - mountPath: /home/locust
            name: locust-scripts
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        resources:
          requests:
            memory: "1Gi"
            cpu: "1"
          limits:
            memory: "1Gi"
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
      - name: locust-scripts
        configMap:
          name: locust-script-cm
---
apiVersion: v1
kind: Service
metadata:
  labels:
    role: locust-master
  name: locust-master
spec:
  type: ClusterIP
  ports:
  - port: 5557
    name: master-bind-host
  - port: 5558
    name: master-bind-host-1
  selector:
    role: locust-master
    app: locust-master
---
apiVersion: v1
kind: Service
metadata:
  labels:
    role: locust-ui
  name: locust-ui
spec:
  type: LoadBalancer
  ports:
  - port: 8089
    targetPort: 8089
    name: web-ui
  selector:
    role: locust-master
    app: locust-master
</code></pre>

<h3>Step 2 : Scale</h3>

<p>If you want to scale more worker to support more requests per seconds, you can do that by just scaling up the worker pods</p>

<pre><code>kubectl scale --replicas=5 deploy/locust-worker -n locust
</code></pre>

<pre><code>locust-master-74c9f6db7c-klk4l   1/1     Running   0          18m
locust-worker-6674d66d5-kgxlp    1/1     Running   0          6s
locust-worker-6674d66d5-m8bdf    1/1     Running   0          6s
locust-worker-6674d66d5-r9v7p    1/1     Running   0          6s
locust-worker-6674d66d5-z2w4x    1/1     Running   0          6s
locust-worker-6674d66d5-zfswz    1/1     Running   0          19m
</code></pre>
]]></content>
  </entry>
  
</feed>
