<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Kubernetes | A D Vishnu Prasad]]></title>
  <link href="http://advishnuprasad.github.io/blog/categories/kubernetes/atom.xml" rel="self"/>
  <link href="http://advishnuprasad.github.io/"/>
  <updated>2023-06-07T18:52:34+05:30</updated>
  <id>http://advishnuprasad.github.io/</id>
  <author>
    <name><![CDATA[A D Vishnu Prasad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Understanding Kubernetes Update Strategies for Deployments]]></title>
    <link href="http://advishnuprasad.github.io/blog/2022/10/05/understanding-kubernetes-update-strategies-for-deployments/"/>
    <updated>2022-10-05T10:21:47+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2022/10/05/understanding-kubernetes-update-strategies-for-deployments</id>
    <content type="html"><![CDATA[<h2>Introduction:</h2>

<p>Kubernetes has revolutionized the way we deploy and manage applications in modern infrastructure. With its robust orchestration capabilities, Kubernetes offers various strategies to update deployments seamlessly, ensuring high availability and minimal downtime. In this blog post, we will delve into the different update strategies available in Kubernetes and explore their benefits and use cases.</p>

<h2>Rolling Update Strategy:</h2>

<p>The Rolling Update strategy is the default approach used by Kubernetes for deployments. It facilitates the gradual replacement of old instances with new ones, minimizing service disruptions. Rolling updates allow you to define the maximum number of pods unavailable at any given time, ensuring the availability and stability of your application throughout the update process.
Advantages:</p>

<p>Continuous availability of the application during the update process.
Rollbacks can be easily performed by reverting to the previous version.
Fine-grained control over the update process, enabling you to set parameters like the maximum number of pods unavailable and the update speed.</p>

<h3>Use Cases:</h3>

<p>Deployments that require high availability and minimal downtime.
Applications with a large number of replicas, where updating all instances simultaneously is not feasible.</p>

<h2>Blue-Green Deployment:</h2>

<p>Blue-Green deployment is a strategy that involves running two identical environments (blue and green) in parallel. The active traffic is routed to the blue environment while the green environment is updated with the new version. Once the update is complete, traffic is switched to the green environment, making it the active production environment.</p>

<h3>Advantages:</h3>

<p>Reduced downtime as the switch between environments is instantaneous.
Easy rollback by directing traffic back to the blue environment in case of issues.
Effective testing of new versions before making them live.
Use Cases:</p>

<p>Critical applications with zero tolerance for downtime.
Extensive testing requirements before updating the production environment.</p>

<h2>Canary Release:</h2>

<p>Canary release strategy involves deploying a new version of an application to a small subset of users or traffic while keeping the majority on the older version. This allows for gradual monitoring and validation of the new version&rsquo;s performance and stability before rolling it out to the entire user base.
Advantages:</p>

<p>Early identification of issues or bugs in the new version.
Controlled release of new features or updates to a subset of users.
Reduced impact on the overall application in case of problems with the new version.</p>

<h3>Use Cases:</h3>

<p>Applications with a large user base, where it is essential to minimize the risk of widespread issues.
New feature releases that need to be tested and validated before reaching the entire user base.</p>

<h2>Conclusion:</h2>

<p>Kubernetes provides multiple update strategies for deployments, each catering to different requirements and scenarios. Whether you opt for the Rolling Update, Blue-Green Deployment, or Canary Release strategy, it is crucial to consider factors such as downtime tolerance, testing requirements, and the size of the user base. By choosing the appropriate update strategy and leveraging Kubernetes' powerful orchestration capabilities, you can ensure smooth and efficient updates while maintaining the availability and stability of your applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimizing Container Management With Elastic Container Registry Lifecycle Policies]]></title>
    <link href="http://advishnuprasad.github.io/blog/2022/06/04/ecr-lifecycle-policy/"/>
    <updated>2022-06-04T13:34:48+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2022/06/04/ecr-lifecycle-policy</id>
    <content type="html"><![CDATA[<h3>Introduction:</h3>

<p>In recent years, containerization has revolutionized software development and deployment. With the rise of container orchestration platforms like Kubernetes, managing containerized applications has become more streamlined and scalable. However, as the number of containers and container images grows, so does the need for efficient management and storage optimization. This is where Elastic Container Registry (ECR) lifecycle policies come into play. In this blog post, we will explore how ECR lifecycle policies can help you streamline your container management process and optimize storage costs.</p>

<h3>Understanding Elastic Container Registry Lifecycle Policies:</h3>

<p>Elastic Container Registry (ECR) is a fully managed container registry service provided by AWS. It enables developers to store, manage, and deploy container images securely. ECR lifecycle policies allow you to define rules that automatically manage the lifecycle of your container images. These policies help you automate the process of cleaning up unused or outdated images, reducing storage costs, and ensuring efficient resource utilization.</p>

<h3>Key Benefits of ECR Lifecycle Policies:</h3>

<ol>
<li>Storage Cost Optimization: ECR lifecycle policies enable you to define rules for image expiration and deletion. By automatically removing unused or outdated images, you can free up storage space and reduce costs associated with long-term image storage.</li>
<li>Streamlined Image Management: With lifecycle policies, you can automate the process of image clean-up based on specific criteria. This ensures that only the necessary images are retained in your registry, making it easier to manage and locate relevant images.</li>
<li>Enhanced Security and Compliance: ECR lifecycle policies allow you to enforce security and compliance measures by defining rules for image retention and expiration. You can ensure that only approved and up-to-date images are stored, reducing the risk of vulnerabilities from outdated or compromised images.</li>
<li>Simplified Development Workflow: By automatically cleaning up unused images, you can improve the efficiency of your development workflow. Developers can focus on the latest versions of images, reducing confusion and potential errors caused by outdated or conflicting images.</li>
</ol>


<h3>Implementing ECR Lifecycle Policies:</h3>

<p>To implement ECR lifecycle policies, you need to define a set of rules using the Amazon ECR Lifecycle Policy Language. The language allows you to specify the conditions for image selection and the actions to be taken, such as expiring or deleting images.</p>

<p>The following are some examples of rules you can define using ECR Lifecycle Policy Language:</p>

<ol>
<li>Expiration Rules: You can set expiration rules based on image tags, image age, or a combination of both. For example, you can define a policy to delete images with a specific tag that are older than 30 days.</li>
<li>Count-Based Rules: You can define rules based on the number of images to retain. For instance, you can specify a policy to retain the five most recent images and automatically delete older images.</li>
<li>Image Tag Rules: You can define rules based on image tags to manage image versions. For example, you can configure a policy to delete images tagged as &ldquo;dev&rdquo; or &ldquo;test&rdquo; after a specific period, while retaining images with the &ldquo;prod&rdquo; tag indefinitely.</li>
<li>Repository-Wide Rules: You can define rules that apply to an entire repository, ensuring consistent image management across all images in the repository.</li>
</ol>


<h3>Sample Terrform Policy Script</h3>

<pre><code>variable "my_ecs_repos" {
  type = map(string)
  default = {
    "staging_web"    = "staging_web",
    "staging_api"    = "staging_api"
  }
}

resource "aws_ecr_lifecycle_policy" "Removeoldimages" {
  for_each   = var.ecs_repos
  repository = each.key

  policy = &lt;&lt;EOF
    {
      "rules": [
        {
          "action": {
            "type": "expire"
          },
          "selection": {
            "countType": "imageCountMoreThan",
            "countNumber": 10,
            "tagStatus": "tagged",
            "tagPrefixList": [
              "${each.value}"
            ]
          },
          "description": "Remove old images",
          "rulePriority": 1
        }
      ]
    }
  EOF

  lifecycle {
    prevent_destroy = true
  }
}
</code></pre>

<h3>Conclusion:</h3>

<p>Elastic Container Registry lifecycle policies offer a powerful tool for automating container image management and optimizing storage costs. By defining rules based on image age, tags, or count, you can ensure efficient resource utilization, enhanced security, and simplified development workflows. With ECR lifecycle policies, you can focus on deploying and maintaining the most relevant and secure container images, reducing overhead and improving your overall container management process.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cloud Agnostic Microservice Deployment Architecture Using Kubernetes]]></title>
    <link href="http://advishnuprasad.github.io/blog/2021/07/27/cloud-agnostic-microservice-deployment-using-kubernetes/"/>
    <updated>2021-07-27T08:46:34+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2021/07/27/cloud-agnostic-microservice-deployment-using-kubernetes</id>
    <content type="html"><![CDATA[<p>A typical web app contains the following microservices.</p>

<blockquote><ul>
<li>API (Flask / FastAPI / Rails)</li>
<li>Worker (Celery / Sidekiq)</li>
<li>Frontend (React / Angular)</li>
<li>Redis (Cache)</li>
<li>Postgres (Database)</li>
</ul>
</blockquote>

<p>We will use kubernetes to deploy these microservices.</p>

<p>We won&rsquo;t use s3 for frontend deployment even though it is easy to deploy cloudfront + s3 combination.
But then we would have a dependency on s3 which is only available in AWS.
(You should use s3 if you know AWS is going to be your only cloud)</p>

<h3>Architecture</h3>

<p><img src="/images/k8s-arch.png" alt="Architecture" /></p>

<p>Using this design, you just need kubernetes manifests. You can use the same in any cloud providers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Upgrade Kubernetes Version in AWS EKS]]></title>
    <link href="http://advishnuprasad.github.io/blog/2019/09/26/upgrade-kubernetes-version-in-eks/"/>
    <updated>2019-09-26T23:55:25+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2019/09/26/upgrade-kubernetes-version-in-eks</id>
    <content type="html"><![CDATA[<p>There are two things you need to do to upgrade Kubernetes in EKS.</p>

<p>Unlike <code>Azure AKS</code> where you can upgrade with a single button, in <code>AWS EKS</code> you need
to upgrade the cluster and worker nodes separately.</p>

<h3>Upgrade Cluster</h3>

<p>=> Login into AWS console.</p>

<p>=> Go to EKS services.</p>

<p>=> Click on Clusters</p>

<p><img src="/images/update_cluster.jpg" alt="Cluster Upgrade" /></p>

<h3>Upgrade Worker Nodes</h3>

<p>Reference Link: <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-stack.html">https://docs.aws.amazon.com/eks/latest/userguide/update-stack.html</a></p>

<h4>Step 1:</h4>

<p>Go to Worker node cloud formation stack and click on <code>Update</code></p>

<h4>Step 2:</h4>

<p>Click on &ldquo;Replace current template&rdquo;</p>

<p>Paste the following URL: <a href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-09-17/amazon-eks-nodegroup.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-09-17/amazon-eks-nodegroup.yaml</a></p>

<p>Click &ldquo;Next&rdquo;</p>

<p><img src="/images/update_stack_1.jpg" alt="Worker Node 1" /></p>

<h4>Step 3:</h4>

<p>Update the following parameters
<code>
NodeAutoScalingGroupDesiredCapacity = NodeAutoScalingGroupDesiredCapacity
</code></p>

<pre><code>NodeAutoScalingGroupMaxSize = NodeAutoScalingGroupDesiredCapacity + 1 (or more)
(It must be more than what you have)
</code></pre>

<pre><code>NodeImageIdSSMParam = /aws/service/eks/optimized-ami/`1.14`/amazon-linux-2/recommended/image_id
(If your cluster version is 1.13 change it to 1.13. Ideally it should match with the Cluster version. Otherwise the upgrade won't work.
You will end up in `matchnodeselector` error and `NotReady` status)
</code></pre>

<p>Leave the other fields to default</p>

<p>Click &ldquo;Next&rdquo;</p>

<p><img src="/images/update_stack_2.jpg" alt="Worker Node 2" /></p>

<h4>Step 4:</h4>

<p>Go to last page and check the box. Click &ldquo;Update&rdquo;</p>

<p><img src="/images/update_stack_3.jpg" alt="Worker Node 3" /></p>

<p>Once the stack is updated, Connect to your cluster. You should see your version upgraded to 1.14</p>

<p><code>kubectl get nodes</code></p>

<p><img src="/images/final.jpg" alt="Final" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes- Elasticsearch Scheduled Backup and Retention Using Curator]]></title>
    <link href="http://advishnuprasad.github.io/blog/2018/10/07/kubernetes-elasticsearch-scheduled-backup-and-retention-using-curator/"/>
    <updated>2018-10-07T21:54:58+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2018/10/07/kubernetes-elasticsearch-scheduled-backup-and-retention-using-curator</id>
    <content type="html"><![CDATA[<p>This post will guide you to create a scheduled elasticsearch backup using curator in k8s cluster.</p>

<h4>Assumptions:</h4>

<pre><code>You already have a 6.x.x elasticsearch k8s cluster
You already have a backup system
You are some what familiar with Kubernetes
You are familiar with curator configs
</code></pre>

<h4>Step 1: Create a configmap with curator yml</h4>

<pre><code>Action 1 will create a snapshot
Action 2 will keep last 20 days snapshots and delete that are older than 20 days
</code></pre>

<p>curator.yml
<code>
apiVersion: v1
kind: ConfigMap
metadata:
  name: curator-config
  labels:
    app: curator
data:
  backup.yml: |-
    ---
    # Remember, leave a key empty if there is no value.  None will be a string,
    # not a Python "NoneType"
    #
    # Also remember that all examples have 'disable_action' set to True.  If you
    # want to use this action as a template, be sure to set this to False after
    # copying it.
    actions:
      1:
        action: snapshot
        description: "Create snapshot"
        options:
          repository: "primary_backup"
          continue_if_exception: False
          wait_for_completion: True
        filters:
          - filtertype: pattern
            kind: regex
            value: ".*$"
      2:
        action: delete_snapshots
        description: &gt;-
          Delete snapshots from the selected repository older than 10 days
          (based on creation_date), for 'curator-' prefixed snapshots.
        options:
          repository: "primary_backup"
          disable_action: False
        filters:
          - filtertype: pattern
            kind: prefix
            value: curator-
            exclude:
          - filtertype: age
            source: creation_date
            direction: older
            unit: days
            unit_count: 20
  config.yml: |-
    ---
    # Remember, leave a key empty if there is no value.  None will be a string,
    # not a Python "NoneType"
    client:
      hosts:
        - elasticsearch
      port: 9200
      url_prefix:
      use_ssl: False
      certificate:
      client_cert:
      client_key:
      ssl_no_validate: False
      http_auth:
      timeout: 30
      master_only: False
    logging:
      loglevel: DEBUG
</code></p>

<h4>Step 2: Create a config map in your kubernetes cluster</h4>

<pre><code>kubectl apply -f curator.yml
</code></pre>

<h4>Step 3: Create a cronjob</h4>

<p>es_cronjob.yml</p>

<pre><code>apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: esbackup
  labels:
    app: esbackup
spec:
  schedule: "@daily"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 3
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 120
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - image: bobrik/curator:5.5.4
            name: curator
            args: ["--config", "/etc/config/config.yml", "/etc/config/backup.yml"]
            volumeMounts:
            - name: config
              mountPath: /etc/config
          volumes:
          - name: config
            configMap:
              name: curator-config
          restartPolicy: Never
</code></pre>

<h4>Step 4: Create cronjob in kubernetes</h4>

<p>This cronjob will run everyday 00:00 hrs
<code>
kubectl create -f es_cronjob.yml
</code></p>

<p>Reference:</p>

<ul>
<li><a href="https://medium.com/@hagaibarel/running-curator-as-a-kubernetes-cronjob-19eaab9afd3b">https://medium.com/@hagaibarel/running-curator-as-a-kubernetes-cronjob-19eaab9afd3b</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/delete_snapshots.html">https://www.elastic.co/guide/en/elasticsearch/client/curator/current/delete_snapshots.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
