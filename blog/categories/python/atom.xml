<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | A D Vishnu Prasad]]></title>
  <link href="http://advishnuprasad.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://advishnuprasad.github.io/"/>
  <updated>2024-02-23T12:00:53+05:30</updated>
  <id>http://advishnuprasad.github.io/</id>
  <author>
    <name><![CDATA[A D Vishnu Prasad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Locust in Kubernetes for Performance Testing: A Powerful Combination]]></title>
    <link href="http://advishnuprasad.github.io/blog/2024/02/23/locust-in-kubernetes/"/>
    <updated>2024-02-23T11:48:08+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2024/02/23/locust-in-kubernetes</id>
    <content type="html"><![CDATA[<p>Performance testing ensures your applications gracefully handle the anticipated—and even the unanticipated—demands of real-world use. Locust, an open-source load testing framework, and Kubernetes, a container orchestration system, offer a potent mix for streamlining and scaling your performance testing efforts.</p>

<h3>Why Locust?</h3>

<p><strong>Python Power</strong>: Locust uses Python to define user behavior, making test creation intuitive and flexible. If you can code it, Locust can simulate it.</p>

<p><strong>Distributed Testing</strong>: Effortlessly simulate massive numbers of concurrent users by running Locust in a distributed mode.</p>

<p><strong>Elegant Web UI</strong>: Monitor your tests in real-time, gain deep insights into performance metrics, and identify bottlenecks thanks to Locust&rsquo;s user-friendly interface.</p>

<h3>Why Kubernetes?</h3>

<p><strong>Scalability</strong>: Seamlessly provision and manage the resources your Locust deployment needs. Spin up workers as required and scale down when testing is complete.</p>

<p><strong>Resilience</strong>: Kubernetes protects your test environment. If a worker node goes down, it automatically restarts pods, minimizing test disruption.</p>

<p><strong>Portability</strong>: Replicate your Locust test infrastructure across different environments (testing, staging, production) with ease.</p>

<p><img src="/images/locust.jpg" alt="Locust UI" /></p>

<h3>Step 1 : Create kubernetes manifests</h3>

<pre><code>---
apiVersion: v1
kind: ConfigMap
metadata:
  name: locust-script-cm
data:
  locustfile.py: |
    from locust import HttpUser, between, task
    import time


    class Quickstart(HttpUser):
        wait_time = between(1, 5)

        @task
        def google(self):
            self.client.request_name = "google"
            self.client.get("https://google.com/")

        @task
        def microsoft(self):
            self.client.request_name = "microsoft"
            self.client.get("https://microsoft.com/")

        @task
        def facebook(self):
            self.client.request_name = "facebook"
            self.client.get("https://facebook.com/")
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  labels:
    role: locust-master
    app: locust-master
  name: locust-master
spec:
  replicas: 1
  selector:
    matchLabels:
      role: locust-master
      app: locust-master
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        role: locust-master
        app: locust-master
    spec:
      containers:
      - image: locustio/locust
        imagePullPolicy: Always
        name: master
        args: ["--master"]
        volumeMounts:
          - mountPath: /home/locust
            name: locust-scripts
        ports:
        - containerPort: 5557
          name: bind
        - containerPort: 5558
          name: bind-1
        - containerPort: 8089
          name: web-ui
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
      - name: locust-scripts
        configMap:
          name: locust-script-cm
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  labels:
    role: locust-worker
    app: locust-worker
  name: locust-worker
spec:
  replicas: 1 # Scale it as per your need
  selector:
    matchLabels:
      role: locust-worker
      app: locust-worker
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        role: locust-worker
        app: locust-worker
    spec:
      containers:
      - image: locustio/locust
        imagePullPolicy: Always
        name: worker
        args: ["--worker", "--master-host=locust-master"]
        volumeMounts:
          - mountPath: /home/locust
            name: locust-scripts
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        resources:
          requests:
            memory: "1Gi"
            cpu: "1"
          limits:
            memory: "1Gi"
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
      - name: locust-scripts
        configMap:
          name: locust-script-cm
---
apiVersion: v1
kind: Service
metadata:
  labels:
    role: locust-master
  name: locust-master
spec:
  type: ClusterIP
  ports:
  - port: 5557
    name: master-bind-host
  - port: 5558
    name: master-bind-host-1
  selector:
    role: locust-master
    app: locust-master
---
apiVersion: v1
kind: Service
metadata:
  labels:
    role: locust-ui
  name: locust-ui
spec:
  type: LoadBalancer
  ports:
  - port: 8089
    targetPort: 8089
    name: web-ui
  selector:
    role: locust-master
    app: locust-master
</code></pre>

<h3>Step 2 : Scale</h3>

<p>If you want to scale more worker to support more requests per seconds, you can do that by just scaling up the worker pods</p>

<pre><code>kubectl scale --replicas=5 deploy/locust-worker -n locust
</code></pre>

<pre><code>locust-master-74c9f6db7c-klk4l   1/1     Running   0          18m
locust-worker-6674d66d5-kgxlp    1/1     Running   0          6s
locust-worker-6674d66d5-m8bdf    1/1     Running   0          6s
locust-worker-6674d66d5-r9v7p    1/1     Running   0          6s
locust-worker-6674d66d5-z2w4x    1/1     Running   0          6s
locust-worker-6674d66d5-zfswz    1/1     Running   0          19m
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Elasticsearch Clone Indices Using Python Client]]></title>
    <link href="http://advishnuprasad.github.io/blog/2022/12/12/elasticsearch-clone-indices-using-python-client/"/>
    <updated>2022-12-12T18:43:08+05:30</updated>
    <id>http://advishnuprasad.github.io/blog/2022/12/12/elasticsearch-clone-indices-using-python-client</id>
    <content type="html"><![CDATA[<h3>Introduction</h3>

<p>Creating a script to automate the cloning of indices from a staging Elasticsearch cluster is a practical approach to streamline the process. By automating this task, you can save time and ensure consistency in your feature environment setup.</p>

<p>Here&rsquo;s an example of how you can write a script to clone indices from your staging Elasticsearch cluster:</p>

<h3>Script: Option 1</h3>

<pre><code>import json
import logging
from typing import Dict

from elasticsearch import Elasticsearch

logging.basicConfig(filename="es.log", level=logging.INFO)

# Creates indices in ES Cloud

class EsClient:
    def __init__(self):
        self.es_client = Elasticsearch(
            ["https://es-url"],
            http_auth=("elastic", "dummy")
        )
        logging.info(self.es_client.ping())

    def copy_mapping(self, source_index, destination_index):

      # Get the mapping and settings of the source index
      source_mapping = self.es_client.indices.get_mapping(index=source_index)
      source_settings = self.es_client.indices.get_settings(index=source_index)

      # Important: The settings brings some unwanted values as well. So pop them before creating
      source_settings[source_index]["settings"]["index"].pop("creation_date")
      source_settings[source_index]["settings"]["index"].pop("provided_name")
      source_settings[source_index]["settings"]["index"].pop("uuid")
      source_settings[source_index]["settings"]["index"].pop("version")

      # Create the destination index with the mapping and settings of the source index
      response = self.es_client.indices.create(
          index=destination_index,
          body={
              "mappings": source_mapping[source_index]["mappings"],
              "settings": source_settings[source_index]["settings"]
          }
      )

    def copy_data(self, source_index, destination_index):

      print(f"Copying data from index {source_index} to {destination_index}")

      # Create a search query to retrieve all documents from the source index
      search_body = {
        "query": {
          "match_all": {}
        }
      }

      # Scroll through the search results and bulk index the documents into the destination index
      scroll_size = 1000
      scroll_response = self.es_client.search(
          index=source_index,
          scroll="2m",
          size=scroll_size,
          body=search_body
      )

      scroll_id = scroll_response['_scroll_id']
      hits = scroll_response['hits']['hits']

      while hits:
          bulk_body = []
          for hit in hits:
              bulk_body.append({"index": {"_index": destination_index, "_id": hit["_id"]}})
              bulk_body.append(hit["_source"])

          self.es_client.bulk(body=bulk_body, refresh=True)
          scroll_response = self.es_client.scroll(scroll_id=scroll_id, scroll="2m")
          scroll_id = scroll_response['_scroll_id']
          hits = scroll_response['hits']['hits']


indices = {"source_index_name": "destination_index_name"}

for key in indices:
    # print(indices[key])
    EsClient().copy_mapping(key, indices[key])
    EsClient().copy_data(key, indices[key])
</code></pre>

<h3>Script: Option 2</h3>

<pre><code>from elasticsearch import Elasticsearch

# Elasticsearch configuration
staging_host = 'staging_cluster_host'
staging_port = 9200

feature_host = 'feature_cluster_host'
feature_port = 9200

# Connect to Elasticsearch clusters
staging_client = Elasticsearch([{'host': staging_host, 'port': staging_port}])
feature_client = Elasticsearch([{'host': feature_host, 'port': feature_port}])

# List all indices in the staging cluster
staging_indices = staging_client.indices.get_alias().keys()

# Clone indices to the feature cluster
for index in staging_indices:
    # Retrieve index settings and mappings from the staging cluster
    index_settings = staging_client.indices.get_settings(index)[index]['settings']
    index_mappings = staging_client.indices.get_mapping(index)[index]['mappings']

    index_settings[index]["settings"]["index"].pop("creation_date")
    index_settings[index]["settings"]["index"].pop("provided_name")
    index_settings[index]["settings"]["index"].pop("uuid")
    index_settings[index]["settings"]["index"].pop("version")

    # Create the index with the same settings and mappings in the feature cluster
    feature_client.indices.create(index=index, body={'settings': index_settings, 'mappings': index_mappings})

    # Reindex data from the staging index to the feature index
    reindex_body = {
        'source': {'remote': {'host': staging_host, 'port': staging_port}, 'index': index},
        'dest': {'index': index}
    }
    feature_client.reindex(body=reindex_body, request_timeout=3600)  # Increase timeout if needed

    print(f"Index '{index}' cloned to the feature cluster.")

print("Index cloning completed.")
</code></pre>
]]></content>
  </entry>
  
</feed>
